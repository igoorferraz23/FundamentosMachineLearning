{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7caac5f4",
   "metadata": {},
   "source": [
    "## Conceitos e Técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71614776",
   "metadata": {},
   "source": [
    "São conceitos e técnicas utilizadas para dar apoio ao aprendizado de maquina, buscando uma melhor performance, avaliar melhor os modelos, melhorar as features entre outros.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349d3cde",
   "metadata": {},
   "source": [
    "### Validação cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6044eb0e",
   "metadata": {},
   "source": [
    "Validação cruzada é uma técnica usada em aprendizado de máquina e estatística para avaliar o desempenho de um modelo de forma mais confiável. Ela funciona dividindo o conjunto de dados em várias partes e repetindo o processo de treino e teste várias vezes, de modo que o modelo seja avaliado em dados diferentes daqueles usados no treinamento.\n",
    "\n",
    "Essa abordagem reduz o risco de overfitting, aproveita melhor conjuntos de dados pequenos e fornece uma estimativa mais estável do desempenho real do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e5aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Obtenção dos dados \"\"\"\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Carregar o dataset Iris\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d1b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia em cada fold: [0.96666667 1.         0.93333333 0.96666667 1.        ]\n",
      "Acurácia média: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Treinando o modelo \"\"\"\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Criar o modelo\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Aplicar validação cruzada (k = 5)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "# Resultados\n",
    "print(\"Acurácia em cada fold:\", scores)\n",
    "print(\"Acurácia média:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d928fa",
   "metadata": {},
   "source": [
    "Com base nos dados acima, o modelo foi treinado e testado várias vezes utilizando diferentes sub-conjuntos dos dados, chamados de **folds**.\n",
    "\n",
    "A acurácia média é calculada a partir dos resultados de todos os folds, fornecendo assim uma estimativa mais confiável da capacidade de **generalização** do modelo, ajudando a reduzir o risco de **overfitting**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488299dd",
   "metadata": {},
   "source": [
    "Outra abordagem relacionada à **Validação Cruzada** é o uso do **GridSearchCV**, que utiliza validação cruzada para avaliar o modelo enquanto testa diferentes combinações de **hiperparâmetros**, selecionando aquelas que apresentam melhor desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5532db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros: {'C': 1, 'solver': 'lbfgs'}\n",
      "Melhor acurácia média (CV): 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Definir o modelo\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Definir a grade de hiperparâmetros\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['lbfgs'],\n",
    "}\n",
    "\n",
    "# Criar o GridSearchCV (cv = 5 folds)\n",
    "grid = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Executar a busca\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Resultados\n",
    "print(\"Melhores hiperparâmetros:\", grid.best_params_)\n",
    "print(\"Melhor acurácia média (CV):\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c5e09",
   "metadata": {},
   "source": [
    "Com base no codigo acima, os melhores **hiperparâmetros** apresentam quais das alternativas apresentadas no **param_grid** retornou a melhor **acurácia**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe897d1a",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3472cfe0",
   "metadata": {},
   "source": [
    "Feature scaling é uma técnica de **pré-processamento** de dados usada em aprendizado de máquina para ajustar a **escala** das features, as colocando em faixas semelhantes de valores. Isso é importante porque algumas variáveis podem ter valores muito maiores que outras (por exemplo, idade vs. salário), o que pode fazer certos algoritmos darem mais importância a elas indevidamente.\n",
    "\n",
    "As formas mais comuns de feature scaling são a normalização (escala os valores geralmente entre 0 e 1) e a padronização (transforma os dados para terem média 0 e desvio padrão 1).\n",
    "\n",
    "Essa técnica melhora muito o desempenho de modelos de regressão linear e de classificações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58eeffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Obtendo dados, separando em teste e treino e aplicando feature scaling \"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# Carregar dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Separar em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Aplicando feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "df_original = pd.DataFrame(X_train, columns=feature_names)\n",
    "df_scaled = pd.DataFrame(X_train_scaled, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ced662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.74</td>\n",
       "      <td>17.91</td>\n",
       "      <td>88.12</td>\n",
       "      <td>585.0</td>\n",
       "      <td>0.07944</td>\n",
       "      <td>0.06376</td>\n",
       "      <td>0.02881</td>\n",
       "      <td>0.01329</td>\n",
       "      <td>0.1473</td>\n",
       "      <td>0.05580</td>\n",
       "      <td>...</td>\n",
       "      <td>15.34</td>\n",
       "      <td>22.46</td>\n",
       "      <td>97.19</td>\n",
       "      <td>725.9</td>\n",
       "      <td>0.09711</td>\n",
       "      <td>0.1824</td>\n",
       "      <td>0.1564</td>\n",
       "      <td>0.06019</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.37</td>\n",
       "      <td>16.39</td>\n",
       "      <td>86.10</td>\n",
       "      <td>553.5</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0.07325</td>\n",
       "      <td>0.08092</td>\n",
       "      <td>0.02800</td>\n",
       "      <td>0.1422</td>\n",
       "      <td>0.05823</td>\n",
       "      <td>...</td>\n",
       "      <td>14.26</td>\n",
       "      <td>22.75</td>\n",
       "      <td>91.99</td>\n",
       "      <td>632.1</td>\n",
       "      <td>0.10250</td>\n",
       "      <td>0.2531</td>\n",
       "      <td>0.3308</td>\n",
       "      <td>0.08978</td>\n",
       "      <td>0.2048</td>\n",
       "      <td>0.07628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.69</td>\n",
       "      <td>13.98</td>\n",
       "      <td>98.22</td>\n",
       "      <td>656.1</td>\n",
       "      <td>0.10310</td>\n",
       "      <td>0.18360</td>\n",
       "      <td>0.14500</td>\n",
       "      <td>0.06300</td>\n",
       "      <td>0.2086</td>\n",
       "      <td>0.07406</td>\n",
       "      <td>...</td>\n",
       "      <td>16.46</td>\n",
       "      <td>18.34</td>\n",
       "      <td>114.10</td>\n",
       "      <td>809.2</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.3635</td>\n",
       "      <td>0.3219</td>\n",
       "      <td>0.11080</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.09208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.91</td>\n",
       "      <td>16.33</td>\n",
       "      <td>82.53</td>\n",
       "      <td>516.4</td>\n",
       "      <td>0.07941</td>\n",
       "      <td>0.05366</td>\n",
       "      <td>0.03873</td>\n",
       "      <td>0.02377</td>\n",
       "      <td>0.1829</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>13.88</td>\n",
       "      <td>22.00</td>\n",
       "      <td>90.81</td>\n",
       "      <td>600.6</td>\n",
       "      <td>0.10970</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>0.1764</td>\n",
       "      <td>0.08235</td>\n",
       "      <td>0.3024</td>\n",
       "      <td>0.06949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.62</td>\n",
       "      <td>23.23</td>\n",
       "      <td>87.19</td>\n",
       "      <td>573.2</td>\n",
       "      <td>0.09246</td>\n",
       "      <td>0.06747</td>\n",
       "      <td>0.02974</td>\n",
       "      <td>0.02443</td>\n",
       "      <td>0.1664</td>\n",
       "      <td>0.05801</td>\n",
       "      <td>...</td>\n",
       "      <td>15.35</td>\n",
       "      <td>29.09</td>\n",
       "      <td>97.58</td>\n",
       "      <td>729.8</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.1517</td>\n",
       "      <td>0.1049</td>\n",
       "      <td>0.07174</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.06953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        13.74         17.91           88.12      585.0          0.07944   \n",
       "1        13.37         16.39           86.10      553.5          0.07115   \n",
       "2        14.69         13.98           98.22      656.1          0.10310   \n",
       "3        12.91         16.33           82.53      516.4          0.07941   \n",
       "4        13.62         23.23           87.19      573.2          0.09246   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.06376         0.02881              0.01329         0.1473   \n",
       "1           0.07325         0.08092              0.02800         0.1422   \n",
       "2           0.18360         0.14500              0.06300         0.2086   \n",
       "3           0.05366         0.03873              0.02377         0.1829   \n",
       "4           0.06747         0.02974              0.02443         0.1664   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.05580  ...         15.34          22.46            97.19   \n",
       "1                 0.05823  ...         14.26          22.75            91.99   \n",
       "2                 0.07406  ...         16.46          18.34           114.10   \n",
       "3                 0.05667  ...         13.88          22.00            90.81   \n",
       "4                 0.05801  ...         15.35          29.09            97.58   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0       725.9           0.09711             0.1824           0.1564   \n",
       "1       632.1           0.10250             0.2531           0.3308   \n",
       "2       809.2           0.13120             0.3635           0.3219   \n",
       "3       600.6           0.10970             0.1506           0.1764   \n",
       "4       729.8           0.12160             0.1517           0.1049   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0               0.06019          0.2350                  0.07014  \n",
       "1               0.08978          0.2048                  0.07628  \n",
       "2               0.11080          0.2827                  0.09208  \n",
       "3               0.08235          0.3024                  0.06949  \n",
       "4               0.07174          0.2642                  0.06953  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dados Originais\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab4bc080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123490</td>\n",
       "      <td>-0.296801</td>\n",
       "      <td>-0.170507</td>\n",
       "      <td>-0.208616</td>\n",
       "      <td>-1.201680</td>\n",
       "      <td>-0.773170</td>\n",
       "      <td>-0.762312</td>\n",
       "      <td>-0.933241</td>\n",
       "      <td>-1.229949</td>\n",
       "      <td>-0.948166</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.197620</td>\n",
       "      <td>-0.506748</td>\n",
       "      <td>-0.307910</td>\n",
       "      <td>-0.273576</td>\n",
       "      <td>-1.507424</td>\n",
       "      <td>-0.449260</td>\n",
       "      <td>-0.572239</td>\n",
       "      <td>-0.840822</td>\n",
       "      <td>-0.856362</td>\n",
       "      <td>-0.765748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.228268</td>\n",
       "      <td>-0.657951</td>\n",
       "      <td>-0.253775</td>\n",
       "      <td>-0.296503</td>\n",
       "      <td>-1.804637</td>\n",
       "      <td>-0.587616</td>\n",
       "      <td>-0.091985</td>\n",
       "      <td>-0.542684</td>\n",
       "      <td>-1.419985</td>\n",
       "      <td>-0.612491</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.422917</td>\n",
       "      <td>-0.458495</td>\n",
       "      <td>-0.465287</td>\n",
       "      <td>-0.438127</td>\n",
       "      <td>-1.273017</td>\n",
       "      <td>0.027042</td>\n",
       "      <td>0.318045</td>\n",
       "      <td>-0.377067</td>\n",
       "      <td>-1.341582</td>\n",
       "      <td>-0.414807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.145534</td>\n",
       "      <td>-1.230564</td>\n",
       "      <td>0.245833</td>\n",
       "      <td>-0.010242</td>\n",
       "      <td>0.519184</td>\n",
       "      <td>1.570006</td>\n",
       "      <td>0.732320</td>\n",
       "      <td>0.386583</td>\n",
       "      <td>1.054201</td>\n",
       "      <td>1.574228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036022</td>\n",
       "      <td>-1.192272</td>\n",
       "      <td>0.203869</td>\n",
       "      <td>-0.127445</td>\n",
       "      <td>-0.024877</td>\n",
       "      <td>0.770802</td>\n",
       "      <td>0.272612</td>\n",
       "      <td>-0.047627</td>\n",
       "      <td>-0.089971</td>\n",
       "      <td>0.488264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.358532</td>\n",
       "      <td>-0.672207</td>\n",
       "      <td>-0.400937</td>\n",
       "      <td>-0.400014</td>\n",
       "      <td>-1.203862</td>\n",
       "      <td>-0.970650</td>\n",
       "      <td>-0.634704</td>\n",
       "      <td>-0.654992</td>\n",
       "      <td>0.096572</td>\n",
       "      <td>-0.827986</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.502189</td>\n",
       "      <td>-0.583287</td>\n",
       "      <td>-0.501000</td>\n",
       "      <td>-0.493386</td>\n",
       "      <td>-0.959895</td>\n",
       "      <td>-0.663496</td>\n",
       "      <td>-0.470142</td>\n",
       "      <td>-0.493515</td>\n",
       "      <td>0.226547</td>\n",
       "      <td>-0.802899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.157472</td>\n",
       "      <td>0.967224</td>\n",
       "      <td>-0.208843</td>\n",
       "      <td>-0.241538</td>\n",
       "      <td>-0.254695</td>\n",
       "      <td>-0.700630</td>\n",
       "      <td>-0.750349</td>\n",
       "      <td>-0.637469</td>\n",
       "      <td>-0.518248</td>\n",
       "      <td>-0.642882</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.195534</td>\n",
       "      <td>0.596414</td>\n",
       "      <td>-0.296107</td>\n",
       "      <td>-0.266734</td>\n",
       "      <td>-0.442374</td>\n",
       "      <td>-0.656085</td>\n",
       "      <td>-0.835138</td>\n",
       "      <td>-0.659802</td>\n",
       "      <td>-0.387208</td>\n",
       "      <td>-0.800613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0    -0.123490     -0.296801       -0.170507  -0.208616        -1.201680   \n",
       "1    -0.228268     -0.657951       -0.253775  -0.296503        -1.804637   \n",
       "2     0.145534     -1.230564        0.245833  -0.010242         0.519184   \n",
       "3    -0.358532     -0.672207       -0.400937  -0.400014        -1.203862   \n",
       "4    -0.157472      0.967224       -0.208843  -0.241538        -0.254695   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0         -0.773170       -0.762312            -0.933241      -1.229949   \n",
       "1         -0.587616       -0.091985            -0.542684      -1.419985   \n",
       "2          1.570006        0.732320             0.386583       1.054201   \n",
       "3         -0.970650       -0.634704            -0.654992       0.096572   \n",
       "4         -0.700630       -0.750349            -0.637469      -0.518248   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0               -0.948166  ...     -0.197620      -0.506748        -0.307910   \n",
       "1               -0.612491  ...     -0.422917      -0.458495        -0.465287   \n",
       "2                1.574228  ...      0.036022      -1.192272         0.203869   \n",
       "3               -0.827986  ...     -0.502189      -0.583287        -0.501000   \n",
       "4               -0.642882  ...     -0.195534       0.596414        -0.296107   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0   -0.273576         -1.507424          -0.449260        -0.572239   \n",
       "1   -0.438127         -1.273017           0.027042         0.318045   \n",
       "2   -0.127445         -0.024877           0.770802         0.272612   \n",
       "3   -0.493386         -0.959895          -0.663496        -0.470142   \n",
       "4   -0.266734         -0.442374          -0.656085        -0.835138   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0             -0.840822       -0.856362                -0.765748  \n",
       "1             -0.377067       -1.341582                -0.414807  \n",
       "2             -0.047627       -0.089971                 0.488264  \n",
       "3             -0.493515        0.226547                -0.802899  \n",
       "4             -0.659802       -0.387208                -0.800613  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dados após feature scaling\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549ce40",
   "metadata": {},
   "source": [
    "Conforme a comparação dos dataframes acima, os valores foram padronizados, sendo esse valor correspondente a formula:\n",
    "$$\n",
    "x_{padronizado} = \\frac{x - \\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03e99f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia SEM scaling: 0.9766\n",
      "Acurácia COM scaling: 0.9825\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Treinando o modelo com e sem feature scaling \"\"\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Regressão Logística SEM feature scaling\n",
    "model_no_scaling = LogisticRegression(max_iter=10000)\n",
    "model_no_scaling.fit(X_train, y_train)\n",
    "\n",
    "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
    "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
    "\n",
    "# Regressão Logística COM feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_scaling = LogisticRegression(max_iter=10000)\n",
    "model_scaling.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
    "acc_scaling = accuracy_score(y_test, y_pred_scaling)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Acurácia SEM scaling: {acc_no_scaling:.4f}\")\n",
    "print(f\"Acurácia COM scaling: {acc_scaling:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87efb641",
   "metadata": {},
   "source": [
    "Podemos ver acima que a acurácia houve uma melhora com scaling, e essa melhora tende a aumentar com datasets mais robustos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
